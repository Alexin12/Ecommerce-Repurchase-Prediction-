# -*- coding: utf-8 -*-
"""part1 git.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tRBHW_JlkhRy6HO3S-0DNm1DRXamG0g8

#1.load raw data
"""

import pandas as pd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import OrderedDict

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/DS班学习资源下载（学生可见）/ecomm
# %cd /content/drive/MyDrive/Colab Notebooks/ecomm/ecomm

ls

aisles = pd.read_csv('aisles.csv') 
departments = pd.read_csv('departments.csv') 
order_products_prior = pd.read_csv('order_products_prior.csv') # ?
order_products_train = pd.read_csv('order_products_train.csv') # ?
orders = pd.read_csv('orders.csv') 
products = pd.read_csv('products.csv')

aisles.head()

departments.head()

order_products_prior.shape

order_products_train.head()

order_products_train.shape

orders.head()

orders.describe()

orders.info()

products.head()

orders.isnull().sum()

print(aisles.shape)
print(departments.shape)
print(order_products_prior.shape)
print(order_products_train.shape)
print(orders.shape)
print(products.shape)

"""#2.data exploration"""

#merge two tabel
prio_details=order_products_prior.merge(orders,on='order_id')
prio_details.head()

prio_details.describe()

#fill weekday number with string
prio_details['order_dow']=prio_details['order_dow'].apply(lambda x: 'Monday' if x==1 else x)
prio_details['order_dow']=prio_details['order_dow'].apply(lambda x: 'Tuesday' if x==2 else x)
prio_details['order_dow']=prio_details['order_dow'].apply(lambda x: 'Wed' if x==3 else x)
prio_details['order_dow']=prio_details['order_dow'].apply(lambda x: 'Thrusday' if x==4 else x)
prio_details['order_dow']=prio_details['order_dow'].apply(lambda x: 'Friday' if x==5 else x)
prio_details['order_dow']=prio_details['order_dow'].apply(lambda x: 'Sat' if x==6 else x)
prio_details['order_dow']=prio_details['order_dow'].apply(lambda x: 'Sunday' if x==0 else x)

prio_details.head()

"""##2.1visualization"""

sns.countplot(x='order_dow',data=prio_details)
plt.xlabel('weekday')
plt.ylabel('order_number')
plt.title('order_frequency')
#more people tend to shop online on Sunday

#order the bar with weekday 
sns.countplot(x='order_dow',data=prio_details,
              order=['Monday','Tuesday','Wed','Thrusday','Friday','Sat','Sunday'])
plt.xlabel('weekday')
plt.ylabel('order_number')
plt.title('order_frequency')
plt.show()

order_hours_counts = orders.groupby("order_id")["order_hour_of_day"].mean()

#get a general idea about the time of shopping online 
order_hours_counts

user_order_hour_mean = orders.groupby("order_id")["order_hour_of_day"].mean().reset_index()

user_order_hour_mean

#calculate how many order are placed in each hour of the day
hour_counts=user_order_hour_mean['order_hour_of_day'].value_counts()

hour_counts.index

#visualize the result of purchasing frequency for each hour of the day
sns.barplot(x=hour_counts.index,y=hour_counts.values)
plt.title('order frequency per hour')
plt.xlabel('hour')
plt.ylabel('number of order')
plt.show()

prio_details.head()

reorder=prio_details.groupby(['reordered'])['days_since_prior_order'].mean()
#get average interval between two purchase for group of repurchased and not repurchased customer

reorder.index

reorder

sns.barplot(x=reorder.index,y=reorder.values)
plt.ylabel('mean_day since last order')

prio_details['user_id'].nunique()

prio_details['order_id'].nunique()

prio_details[prio_details['reordered']==0][['days_since_prior_order','reordered']]

reorder_rate=prio_details.groupby(['order_dow','order_hour_of_day'])['reordered'].mean().reset_index()

reorder_rate

reorder_rate=reorder_rate.pivot(index='order_dow', columns='order_hour_of_day',values='reordered')

plt.figure(figsize=(12,6))
sns.heatmap(reorder_rate,cmap="Blues")
plt.title("Reorder Ratio")
plt.ylabel("")
plt.xlabel("Hours")

plt.show()

"""##2.2 data quick check"""

#check if all dataset have consistant record 

print("Value size of the order dataset: ", orders.shape[0])
print("NaN count in days_since_prior_order column: ", orders[orders.days_since_prior_order.isnull()].shape[0])

print("order_number 1 count in orders table: ", orders[orders.order_number == 1].drop_duplicates().shape[0])
print("user_id count in orders table: ", orders.user_id.drop_duplicates().shape[0])

orders.head()

"""2.2.Validate Valid orders matching in the prior table"""

orders.eval_set.value_counts()

orders.groupby(['eval_set'],as_index=False).agg(OrderedDict([('order_id','nunique')]))
print("order_id from prior found in orders: ", order_products_prior[order_products_prior.order_id.isin(orders.order_id)].order_id.nunique())

print('number of unique rows in prio dataset',order_products_prior['order_id'].nunique())
# print('number of rows prio in order',order_products_prior['order_id'].isin(orders.order_id).count())##会count到重复值 
print('number of rows prio in orderdataset',order_products_prior[order_products_prior.order_id.isin(orders.order_id)].order_id.nunique())

"""2.3. Validate orders matching in the train table"""

print("orders count in train: ", order_products_train['order_id'].nunique())
print("order_id from train found in orders: ", order_products_train[order_products_train.order_id.isin(orders.order_id)].order_id.nunique())

print('unique order id',order_products_train['order_id'].nunique())
print('train_order id in order',order_products_train[order_products_train.order_id.isin(orders.order_id)].order_id.nunique())

"""2.4. Validate the intersection between prior and train table"""

order_products_prior.merge(order_products_train,on='order_id').shape[0]

order_products_prior.merge(order_products_train,on='order_id').shape[1]

""" Validate the user_id matching in prior and train set"""

orders.eval_set.value_counts()#

orders.groupby('eval_set',as_index=False).agg(OrderedDict([('user_id','nunique')]))#make sure user match between two table

priod_user_id=set(orders[orders.eval_set=='prior']['user_id'])
train_user_id=set(orders[orders.eval_set=='train']['user_id'])
print('number of user in prio',len(priod_user_id))
print('number of user in prio',len(train_user_id))

print('same customer in train and prio',len(priod_user_id.intersection(train_user_id)))

"""#3.Feature Engineering"""

aisles = pd.read_csv('aisles.csv')
departments = pd.read_csv('departments.csv')
order_products_prior = pd.read_csv('order_products_prior.csv')
order_products_train = pd.read_csv('order_products_train.csv')
orders = pd.read_csv('orders.csv')
products = pd.read_csv('products.csv')
#reorder -- if customer purchased this product before  label will be 1 else be 0

order_products_train.head()

"""##3.1 Construct Model Label"""

# build a new feature of combination for user_id and product id
train_details = order_products_train.merge(orders, on = 'order_id')
train_user_ids = set(orders[orders['eval_set'] == 'train']['user_id'])
train_unique_key = train_details['user_id'].astype('str') + '_' + train_details['product_id'].astype('str')
#创造一个新的feature

train_unique_key.head()

# (2) 
prior_details = order_products_prior.merge(orders, on = 'order_id')

prior_details.head()

#（3）
model_all_data = prior_details[prior_details.user_id.isin(train_user_ids)][['reordered','user_id','product_id','order_number', 'order_dow', 'order_hour_of_day', 'days_since_prior_order']]
model_all_data = model_all_data.drop_duplicates(subset=['user_id', 'product_id']) 
model_all_data['unique_key'] = model_all_data['user_id'].astype('str') + '_' + model_all_data['product_id'].astype('str')

#（4）
model_all_data['label'] = 0
model_all_data.loc[model_all_data.unique_key.isin(train_unique_key), 'label'] = 1

model_all_data[['reordered','label']].value_counts()

model_all_data.head()

model_all_data.shape

model_all_data=model_all_data.drop(['reordered'],axis=1)

"""##3.2. Construct Model Features

2.1. Feature Group 1: user-product activity features
"""

user_product_features = ['user_product__total_orders',
                         'user_product__add_to_cart_order_mean',
                         'user_product__reordered_mean',
                         'user_product__most_dow',
                         'user_product__most_hod']

prior_details.reordered.value_counts()

prior_details.head()

df_user_product_features=(prior_details.groupby(['product_id','user_id'],as_index=False)
                                           .agg(OrderedDict(
                                                     [('order_id','count'),
                                                      ('add_to_cart_order','mean'),
                                                      ('reordered','mean'),
                                                      ('order_dow',(lambda x:x.mode()[0])),
                                                      ('order_hour_of_day',(lambda x:x.mode()[0]))])))

df_user_product_features.columns=['product_id','user_id']+user_product_features

df_user_product_features.head()

prior_details.loc[(prior_details['user_id']==47549)&(prior_details['product_id']==1),['user_id','product_id','reordered']]

model_all_data = model_all_data.merge(df_user_product_features, on = ['user_id', 'product_id'])#把新的feature 加回去

model_all_data.head()

"""2.2. Feature Group 2: product features """

product_features = ['product__total_orders',
                     'product__add_to_cart_order_mean',
                     'product__total_users',
                     'product__reordered_mean',
                     'product__most_dow',
                     'product__most_hod',
                     'product__days_since_prior_order_mean'
                     ]

df_product_features=(prior_details.groupby(['product_id'],as_index=False)
                                          .agg(OrderedDict(
                                            [('order_id','nunique'),
                                            ('add_to_cart_order','mean'),
                                            ('user_id','nunique'),
                                            ('reordered','mean'),
                                            ('order_dow',(lambda x: x.mode()[0])),
                                            ('order_hour_of_day',(lambda x: x.mode()[0])),
                                            ('days_since_prior_order','mean')
                                            ])))
df_product_features.columns = ['product_id'] + product_features

df_product_features.head()

model_all_data = model_all_data.merge(df_product_features, on = ['product_id'])

model_all_data = model_all_data.merge(products[['product_id','aisle_id', 'department_id']], on = ['product_id'])#只merge product里面特定的几列---

model_all_data.head().shape

model_all_data.head()

model_all_data.rename(columns={'aisle_id': 'product__aisle_id', 'department_id': 'product__department_id'}, inplace=True)

"""2.3. Feature Group 3: user features

"""

user_features = ['user__order_count',
                  'user__product_count',
                  'user__days_since_prior_order_mean',
                  'user__reordered_mean',
                  'user__most_dow',
                  'user__most_hod',
                  ]

df_user_features = (prior_details.groupby(['user_id'],as_index=False)
                                           .agg(OrderedDict(
                                                   [('order_id','nunique'), #total number of orders of customer 
                                                    ('product_id','count'), #total number of products purchase
                                                    ('days_since_prior_order','mean'), #mean interval between purchase
                                                    ('reordered', 'mean'), #reoder rate of customer
                                                    ('order_dow', (lambda x: x.mode()[0])), #most frequent day of place order
                                                    ('order_hour_of_day', (lambda x: x.mode()[0])), #most frequent time of place order
                                                    ])))
df_user_features.columns = ['user_id'] + user_features

model_all_data = model_all_data.merge(df_user_features, on = ['user_id'])

model_all_data['label'].value_counts()

model_all_data.shape#total 29 features after feature engineering

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/Colab Notebooks/ecomm

# Save the model_all_data dataframe as csv file to the file path we selected above.
model_all_data.to_csv('model_all_data.csv', index=False)

"""#4.modeling part

##4.1 load data
"""

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.pipeline import Pipeline 
from imblearn.under_sampling import RandomUnderSampler 
from imblearn.over_sampling import RandomOverSampler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score

import keras
from keras.utils.vis_utils import plot_model
from keras.wrappers.scikit_learn import KerasClassifier

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/Colab Notebooks/ecomm

ls

model_all_data = pd.read_csv('model_all_data.csv')

model_all_data.head()

model_all_data.columns

unique_key = ['user_id', 'product_id']
raw_features = ['order_number',
                     'order_dow',
                     'order_hour_of_day',
                     'days_since_prior_order',
                     'user_product__total_orders',
                     'user_product__add_to_cart_order_mean',
                     'user_product__reordered_mean',
                     'user_product__most_dow',
                     'user_product__most_hod',
                     'product__total_orders',
                     'product__add_to_cart_order_mean',
                     'product__total_users',
                     'product__reordered_mean',
                     'product__most_dow',
                     'product__most_hod',
                     'product__days_since_prior_order_mean',
                     'user__order_count',
                     'user__product_count',
                     'user__days_since_prior_order_mean',
                     'user__reordered_mean',
                     'user__most_dow',
                     'user__most_hod',
                     'product__aisle_id',
                     'product__department_id',
                     ]
label = ['label']

model_all_data = model_all_data[unique_key + raw_features + label]

model_all_data.head()

"""## 4.2  Feature Data Analysis & Tranformation

Label Ratio
"""

label_count = model_all_data.label.value_counts()
print("Negative label count: ", label_count[0])
print("Positive label count: ", label_count[1])
print("Positive label rate is {:.2f}%".format(
      label_count[1] / (label_count[0] + label_count[1]) * 100) )

ax = label_count.sort_values().plot(kind='barh', title='Labels Count')
ax.grid(axis="x")
#balanced data

"""Null value check"""

model_all_data.isna().sum()

"""Categorical Feature Transformation"""

# One-hot encoding for the `product__department_id` feature.
model_all_data = pd.concat(
    [model_all_data.drop('product__department_id',axis=1),
     pd.get_dummies(model_all_data["product__department_id"],
                    prefix='product__department_id_')],
                     axis=1)

"""##4.3 Data partition to Train Validation and Test

In order to do model training, model selection and evaluation, we split `model_all_data` to three parts: train, validation, test.
"""

# We randomly select 20% users as test data. 
# The remaining data will be used as train & validation data.
test_user_ids = model_all_data.user_id.drop_duplicates().sample(frac=0.2)

train_validation_data = model_all_data[~model_all_data.user_id.isin(test_user_ids)]##继续确保 id 是不在tets里的
test_data = model_all_data[model_all_data.user_id.isin(test_user_ids)]

model_all_data.shape

train_validation_data.shape

test_data.shape

"""## 4.4  Model Selection with Cross Validation"""

train_validation_data_x = train_validation_data.drop(['user_id', 'product_id', 'label'],axis=1)
train_validation_data_y = train_validation_data['label']

classifiers = [
    LogisticRegression(),
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    AdaBoostClassifier(),
    GradientBoostingClassifier(),
    ]

def build_ml_pipeline(classifier):
  steps = list()
  steps.append(('fillnan', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0.0)))
  steps.append(('downSample', RandomUnderSampler()))
  steps.append(('scaler', MinMaxScaler()))
  steps.append(('model', classifier))
  pipeline = Pipeline(steps=steps)
  return pipeline

# Commented out IPython magic to ensure Python compatibility.
#print out model performance and latency
for classifier in classifiers:
  pipeline = build_ml_pipeline(classifier)
#   %time scores = cross_val_score(pipeline, train_validation_data_x, train_validation_data_y, cv=5, scoring='f1')
  print(classifier.__class__.__name__, ': F1 value is %.3f (%.3f)' % (np.mean(scores)*100, np.std(scores)*100))
  print('==============')

#use f1 score as metrics

"""## 4.5 Feature Selection
 for speed up model trainning
"""

pipeline = build_ml_pipeline(GradientBoostingClassifier())
pipeline.fit(train_validation_data_x, train_validation_data_y)

importances = pipeline.steps[3][1].feature_importances_
feature_names=train_validation_data_x.columns.tolist()

df_importances = pd.DataFrame(
    {"feature":feature_names, "importance":importances}).sort_values("importance", ascending=False)
top15_features = df_importances.head(15)

# Only use the selected top 15 features in our training data.
train_validation_data_x_select_features = train_validation_data_x[top15_features['feature']]

top15_features

"""##4.6 Train Neural Network Model"""

# Feed forward neural network
def create_model():
    model = keras.Sequential([
        ### layer input
        keras.layers.Dense(30, input_dim=15, activation='relu'),
        ###  layer 1
        keras.layers.Dense(15, activation='relu'),
        ###  layer 2
        keras.layers.Dense(5, activation='relu'),
        ### layer output
        keras.layers.Dense(units=1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy')
    return model

NN_model = KerasClassifier(build_fn=create_model, epochs=64, batch_size=32, verbose=0)

keras_model = create_model()

keras_model = create_model()

print(keras_model.summary())

plot_model(keras_model)

"""4.7  Model Selection with Selected Top 15 Features including Neural Network Model"""

# Add our neural network model to our classifiers list.
classifiers.append(NN_model)

# Commented out IPython magic to ensure Python compatibility.
for classifier in classifiers:
  pipeline = build_ml_pipeline(classifier)
#   %time scores = cross_val_score(pipeline, train_validation_data_x_select_features, train_validation_data_y, cv=5, scoring='f1')
  print(classifier.__class__.__name__, ': F1 value is %.3f (%.3f)' % (np.mean(scores)*100, np.std(scores)*100))
  print('==============')


  # NN_model：
  # F1 = 34.941, slightly lower than gradient boost of 35.177. But, the variance of the NN_model F1 is 1.284.

"""#5.Hyper-parameter tuning for GradientBoostingClassifier"""

sample_user_ids = train_validation_data.user_id.drop_duplicates().sample(frac=0.05)
train_validation_data_sample = train_validation_data[train_validation_data.user_id.isin(sample_user_ids)]

train_validation_data_sample_x = train_validation_data_sample.drop(['user_id', 'product_id', 'label'],axis=1)[top15_features['feature']]
train_validation_data_sample_y = train_validation_data_sample['label']

param_grid = {
    'model__n_estimators': [100,150],
    'model__max_depth': [4,6],
    'model__min_samples_split': [2,4,6],
    'model__learning_rate': [0.005, 0.01],
}

#这一步是需要注意滴 不同的模型 hyperpa 是不样的 
#而且需要知道区间 否则太久

grid = GridSearchCV(build_ml_pipeline(GradientBoostingClassifier()), cv=5, param_grid=param_grid, scoring='f1')
grid.fit(train_validation_data_sample_x, train_validation_data_sample_y)

print("Best F1 value is %.3f" % grid.best_score_)
print("Params are %s" % grid.best_params_)

"""#6. Train the final GradientBoostingClassifier

6.1. Train the final model with all training+validation data and the best hyper-parameters.
"""

pipeline = build_ml_pipeline(GradientBoostingClassifier(
    n_estimators=100,
    min_samples_split=2,
    max_depth=4,
    learning_rate=0.005,
))

pipeline.fit(train_validation_data_x_select_features, train_validation_data_y)

importances = pipeline.steps[3][1].feature_importances_
feature_names=train_validation_data_x_select_features.columns.tolist()
df_importances = pd.DataFrame(
    {"feature":feature_names, "importance":importances}).sort_values("importance", ascending=False)
df_importances.head(15)

"""# 7. Evaluate model on Test Data

7.1. Predict product reorder on Test Data
"""

test_data_x_selected_features = test_data.drop(['user_id', 'product_id', 'label'],axis=1)[top15_features['feature']]
test_data_y = test_data['label']

predict_y = pipeline.predict(test_data_x_selected_features)

df_output = pd.DataFrame(
    {"user_id":test_data.user_id, "product_id":test_data.product_id, "predict":predict_y, "label":test_data.label})

df_output.head()

predict_y_prob = pipeline.predict_proba(test_data_x_selected_features)[:,1]

plot_confusion_matrix(pipeline, test_data_x_selected_features, test_data_y,
                      display_labels=["not reorder","reorder"],cmap=plt.cm.Blues)

acc = accuracy_score(test_data_y, predict_y)
f1 = f1_score(test_data_y, predict_y)
pre = precision_score(test_data_y, predict_y)
rec = recall_score(test_data_y, predict_y)
auc = roc_auc_score(test_data_y, predict_y_prob)

print("Model evaluation result on test data: ")
print("F1 Score : {:.4%}".format(f1))
print("ROC AUC : {:.4%}".format(auc))
print("Accuracy : {:.4%}".format(acc))
print("Precision : {:.4%}".format(pre))
print("Recall : {:.4%}".format(rec))